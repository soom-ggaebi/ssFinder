from pyspark.sql import SparkSession
from config.config import SPARK_MASTER

def test_batch_job():
    spark = None
    try:
        print("âœ… SparkSession ìƒì„± ì‹œì‘...")
        spark = SparkSession.builder \
            .appName("TestBatchJob") \
            .master(SPARK_MASTER) \
            .config("spark.executor.memory", "512m") \
            .config("spark.executor.cores", "1") \
            .config("spark.default.parallelism", "2") \
            .config("spark.driver.host", "172.23.0.2") \
            .config("spark.driver.port", "4041") \
            .config("spark.blockManager.port", "4042") \
            .config("spark.driver.bindAddress", "0.0.0.0") \
            .getOrCreate()

        print("âœ… SparkSession ìƒì„± ì™„ë£Œ!")
        print("ğŸ‘‰ ë§ˆìŠ¤í„° ì£¼ì†Œ:", spark.sparkContext.master)
        print("ğŸ‘‰ ì‹¤í–‰ì¤‘ì¸ ì•± ì´ë¦„:", spark.sparkContext.appName)

        print("ğŸ‘‰ ì—°ê²°ëœ Executors:")
        executors = spark.sparkContext._jsc.sc().getExecutorMemoryStatus().keySet()
        executor_iter = executors.iterator()

        while executor_iter.hasNext():
            executor = executor_iter.next()
            print("  ğŸ”¹", str(executor))


        # ì˜ˆì œ ë°ì´í„°
        summary_data = [("FTEST001", "í…ŒìŠ¤íŠ¸ ë¬¼í’ˆ", "ë¸”ë™", "í…ŒìŠ¤íŠ¸ê²½ì°°ì„œ", 
                         "https://s3.amazonaws.com/bucket/sample_processed.jpg", 
                         "2025-03-15 00:00:00", "í…ŒìŠ¤íŠ¸ > ìƒ˜í”Œ")]

        detail_data = [("FTEST001", "STORED", "ìƒì„¸ í…ŒìŠ¤íŠ¸ ì¥ì†Œ", 
                        "010-1234-5678", "í…ŒìŠ¤íŠ¸ ìƒì„¸ì •ë³´ ë³´ì¶©")]

        summary_schema = "management_id string, name string, color string, stored_at string, s3_image string, found_at string, prdtClNm string"
        detail_schema = "management_id string, status string, location string, phone string, detail string"

        print("ğŸ“¦ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì¤‘...")
        summary_df = spark.createDataFrame(summary_data, schema=summary_schema)
        detail_df = spark.createDataFrame(detail_data, schema=detail_schema)

        print("ğŸ”— ë°ì´í„°í”„ë ˆì„ ì¡°ì¸ ì¤‘...")
        final_df = summary_df.join(detail_df, on="management_id", how="left")

        print("ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")

        print("ğŸ“ˆ Row count ì‹¤í–‰:")
        print("Row count: ", final_df.count())

        spark.stop()

    except Exception as e:
        print("âŒ Spark ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", str(e))

    finally:
        if spark:
            try:
                spark.stop()
                print("ğŸ§¹ Spark ì„¸ì…˜ ì •ìƒ ì¢…ë£Œ")
            except Exception as e:
                print("âš ï¸ Spark ì„¸ì…˜ ì¢…ë£Œ ì‹¤íŒ¨:", str(e))

if __name__ == "__main__":
    test_batch_job()
