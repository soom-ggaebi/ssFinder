# 멀티 스테이지 빌드 적용
FROM bitnami/spark:latest AS builder

USER root
RUN apt-get update && apt-get install -y curl
RUN curl -L -o /opt/bitnami/spark/jars/spark-sql-kafka-0-10_2.12-3.5.5.jar \
    https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.5/spark-sql-kafka-0-10_2.12-3.5.5.jar
RUN apt-get purge -y curl && apt-get autoremove -y && rm -rf /var/lib/apt/lists/*

FROM bitnami/spark:latest

# 빌더 단계에서 다운로드한 JAR 파일 복사
COPY --from=builder /opt/bitnami/spark/jars/spark-sql-kafka-0-10_2.12-3.5.5.jar /opt/bitnami/spark/jars/

# 전체 프로젝트 폴더를 /app로 복사 (Docker context가 프로젝트 루트라고 가정)
WORKDIR /app
COPY . /app

# root 사용자로 전환하여 Python 패키지 설치
USER root
RUN apt-get update -y && \
    apt-get install -y python3-pip && \
    pip3 install --no-cache-dir -r requirements.txt && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# 비-root 사용자로 전환 (Bitnami 기본 사용자)
USER 1001

# 환경변수 (docker-compose 또는 .env 파일에서 오버라이딩 가능)
ENV SPARK_MASTER="spark://spark-master:7077" \
    SPARK_DRIVER_MEMORY="2g" \
    SPARK_EXECUTOR_MEMORY="1g"

# Healthcheck: Spark UI가 4000 포트에서 실행되는지 확인
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -f http://localhost:4000 || exit 1

# PYTHONPATH에 /app 추가하여 전체 프로젝트 폴더를 모듈 검색 경로에 포함
ENV PYTHONPATH=/app

# 진입점: spark-submit으로 Spark 애플리케이션 제출
CMD spark-submit --master ${SPARK_MASTER} \
    --conf "spark.jars.ivy=/tmp/.ivy" \
    --conf "spark.sql.streaming.checkpointLocation=hdfs://${HADOOP_NAMENODE_HOST}:${HADOOP_NAMENODE_PORT}/spark-checkpoints" \
    --conf "spark.ui.port=4000" \
    /app/app/spark/spark_processing.py
